%
% 2
%
\chapter{Datasets}%
	%
	%
	\label{ch:data}%
	\index{Datasets}%
	%
	%
  \begin{mybox}
    %
    \newthought{This section} deals with data management. Because we will be using course datasets and preinstalled Stata commands, you will need to have \textbf{run the course setup} as explained in Section~\ref{sec:course-setup} to follow from here on.%

    %
    \paragraph{Replication coursework} %
    Make sure that you have fully replicated \filename{week2.do} and \filename{week3.do} by the end of this section.%

    %
    \paragraph{Additional readings} %
    Read \citeauthor[ch.~2--3]{Urdan:2010a}, and optionally  \citeauthor[ch.~2.1--2.4]{FeinsteinThomas:2002d}, by the end of this section.%
    
  \end{mybox}
  %
  %
  % \minitoc
  \startcontents[chapters]%
  \printcontents[chapters]{l}{1}{\setcounter{tocdepth}{2}}%
	%
	\newpage
	%
	%
	
% -- fre correctly pkg{}'d on first instance
% -- 2013-03-04

% 0. intro:

\newthought{Quantitative data} is a form of data that simplifies information into variables that take different types and values. Some variables, such as gross domestic product or monthly income, hold \emph{continuous} data that are strictly numeric, while others hold \emph{categorical} data, such as social class for individuals or political regime type for nation states.%

As in qualitative research, the collection of data for quantitative analysis systematically raises issues of conceptualisation, measurement and reliability, with specific issues related to the sampling design that makes a sample representative of a target population, like the residents of a country.

\newthought{Manipulating a dataset} is itself a complex task that requires some familiarity with the structure of the data, with the software commands available to prepare the data, and with the research design of your project. Using predefined datasets, as we will in this course, will simplify these operations a great deal, but will not entirely suppress them, so that we can learn some of the basics of data management and preparation.

This section describes the essential steps that you should follow to prepare your data before starting your analysis. It also outlines some complementary data skills that will serve you if you have to work with quantitative data beyond the scope of this course.

\newthought{A dataset} is a file that contains a set of variables for a given number of \emph{observations}. Each row of the dataset holds a single observation, and each column holds a single variable. The data will usually start with an `ID' or `Name' variable, which might be the country name or code in a country-level dataset, or a unique identifier code in an anonymous individual-level survey. That variable is set to characterise each \emph{unit of analysis} of the data, which might be just anything: countries, firms, NGOs, individuals, historical events, etc.

\bigskip
\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{lllll}
\toprule
Observation & ID & Variable 1 & Variable 2 & ... \\
\midrule
\emph{Country-level data:} & & & & \\
\quad 1. & Afghanistan   & Value   & Value   & ... \\
\quad 2. & Albania & Value & \hlred{\emph{Missing}} & ... \\
\quad 3. & Algeria   & Value   & Value   & ... \\
\quad ... & & & & \\
\addlinespace
\emph{Individual-level data:} & & & & \\
\quad 1. & 17534   & Value   & Value   & ... \\
\quad 2. & 17535 & Value & Value & ... \\
\quad 3. & 17536   & \hlred{\emph{Missing}}   & Value   & ... \\
\quad ... & & & & \\
\bottomrule
\end{tabular}
\caption{Basic data structures, with observations in rows and variables in columns. Note that the variables have some missing values.}
\label{tbl:basic-data}
\end{center}
\end{table}

\bigskip
\newthought{The total number of observations} of a dataset is denoted $N$. It might be small, as with $N = 27$ OECD countries, or huge, as with `big data' where $N > 100,000$. Statistical tests can easily fail to apply when the number of observations is low, typically somewhere around $N < 30$. The data can also suffer from overall sparsity if there are few variables or many missing values. 

Basic data structures like those shown in Table~\ref{tbl:basic-data} are \emph{cross-sectional} because they contain data for only one time period. Datasets that contain values for more than one time period $T$ are called \emph{time series}. Table~\ref{tbl:csts-data} shows an example of cross-sectional time series.

\bigskip
\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{lllll}
\toprule
Observation & ID & Year & Variable 1 & ... \\
\midrule
\quad 1. & France & 2010 & Value & ... \\
\quad 2. & France & 2011 & Value & ... \\
\quad 3. & France & 2012 & Value & ... \\
\quad 4. & Germany & 2010 & Value & ... \\
\quad 5. & Germany & 2011 & \hlred{\emph{Missing}} & ... \\
\quad 6. & Germany & 2012 & Value & ... \\
\quad ... & & & & \\
\bottomrule
\end{tabular}
\caption{Cross-sectional time series, a.k.a. \smallcaps{csts} data. This data structure is common in some research areas like international relations and comparative politics. In a dataset that counts $N = 27$ countries over $T = 20$ years, the total number of country-year observations will be $27 \times 20 = 540$, or less as soon as there are missing values.}
\end{center}
\label{tbl:csts-data}
\end{table}

\bigskip
The structure of the data can get even more complex if there is more than one \emph{level of observation} nested within each other, as when some respondents are taken from the same household, region and country. Furthermore, the observations can repeat over time and form a \emph{panel}, as when epidemiologists observe a cohort of adults through their life course in a longitudinal study. Table~\ref{tbl:panel-data} shows an example of cross-country household panel data.

\bigskip
\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{lllllll}
\toprule
Observation & Country & Year & Household ID & Person ID & Variable 1 & ... \\
\midrule
\quad 1. & Turkey & 2011 & 907 & 24101 & Value & ... \\
\quad 2. & Turkey & 2012 & 907 & 24102 & Value & ... \\
\quad 3. & Turkey & 2011 & 908 & 24103 & Value & ... \\
\quad 4. & Turkey & 2012 & 908 & 24104 & Value & ... \\
\quad 5. & Ukraine & 2011 & 403 & 46889 & Value & ... \\
\quad 6. & Ukraine & 2012 & 403 & 46890 & Value & ... \\
\quad 7. & Ukraine & 2011 & 404 & 46891 & \hlred{\emph{Missing}} & ... \\
\quad 8. & Ukraine & 2012 & 404 & 46892 & \hlred{\emph{Missing}} & ... \\
\quad ... & & & & & & \\
\bottomrule
\end{tabular}
\caption{Panel data, with individuals nested in households nested into countries. The time period covered by panel data can range from a few years for repeated surveys to decades for longitudinal studies of patient cohorts in biomedicine or financial time series holding stock values.}
\end{center}
\label{tbl:panel-data}
\end{table}

\newthought{In this course}, we focus on cross-sectional data in which we can assume that the observations are independent from each other. Special methodological requirements apply to panel data and times series, where this requirement is violated by the temporal dependence between recurring observations.\footcite{BeckKatz:2011e}%

A selection of datasets is available from the \data folder. It contains three individual-level surveys using large probability samples: the \ess (\ESS), \gss (\GSS) and \wvs (\WVS). For country-level analysis, the recommended source is the \qog (\QOG) Standard Dataset, which collates together variables from the World Bank and the United Nations as well as academic research and various other sources. All datasets are high quality sources with extensive documentation.%

The \GSS dataset, just as the \nhis (\NHIS) dataset, which is used primarily for teaching examples, both contain several years of observations and require to be subset to only one year. The \QOG dataset is a cross-sectional version of time series data, and therefore works with a more flexible period of observation that uses multiple-year figures pooled into representative averages. The \ESS and \WVS datasets correspond to survey waves; the course datasets contain Round 4 (2008) of the \ess and Wave 3 (1999-2001) of the \wvs.%

\newthought{For your project}, select one of these datasets and fetch additional documentation from the course appendix at p.~\pageref{sec:data-sources}. You will need to get the data codebook as well as information on the \emph{sampling procedure} if you are using survey data. The sampling procedure is the technique that guarantees the representativeness of the sample by ensuring that each member of the target population had an equal probability of selection within the sampling frame.%

Several techniques can enhance simple random sampling procedures, such as stratified or cluster sampling, where primary sampling units (\eg individuals) are selected within nested structures (\eg households). Weighting schemes can also provide postsurvey adjustment to correct for undercoverage, \ie the existence of different probabilities of selection among the survey respondents. You will find mentions to survey weights in the codebooks of your dataset and in the data appendix, so that you can easily set them in your own analysis.%

\newthought{Using external data for your research project} is \textbf{not} recommended for this course, due to the extra workload that it will inflict on your group at the data preparation level. If you plan to ask for an exception, please make that decision reasonably and plan your work well in advance, especially if you have not yet assembled the data, which is an \emph{extremely} time-consuming task. \hlred{You will be asked to abandon any effort that has not been successful in the first month of class.}%

%
	\label{external-data-warning}%
	\footnote{See \url{https://github.com/briatte/srqm/wiki/data} for a short selection.} %
		might contain possible candidates for cross-sectional analysis. %
		%

You need to read this section to achieve three goals:

\begin{enumerate}
	\item \textbf{Prepare your dataset for analysis.} The code for this operation will form the first segment of your do-file, in which you check the description and encoding of your variables, and possibly recode some of them.
	\item \textbf{Describe the variables.} You will be required to provide summary statistics for continuous variables and relative frequencies (percentages) for categorical ones. More on variable types in a few pages.
	\item \textbf{Preserve a high sample size.} Your data preparation code will end on a diagnosis of how missing values affect your sample, as you will be required to delete all observations that contain missing values.
\end{enumerate}
   
Next to the data itself, you should now turn to the data documentation to start reading about the measurements that you will be manipulating for data preparation.%

%
\include{2A_essential}%
%
\include{2B_additional}%
%
\include{2C_advanced}%
%
\include{2D_exercises}
%

%
% have a nice day
%

\stopcontents[chapters]
